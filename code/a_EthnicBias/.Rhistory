library(data.table) #to read in larger datasets more effectively
library(lfe) # clustered SEs in OLS
library(clusterSEs) # clustered SEs for GLM
library(xtable)
##
## Error: "Read less rows (13299) than were allocated (14148)."
##
## DEH: I think this might be ok
##
## - Same rows for regular CSV call
## dta <- read.csv("instances_mergerd_seattle.csv", header=T, as.is=T)
##
## - This thread suggests the warning sometimes gets thrown incorrectly:
## https://github.com/Rdatatable/data.table/issues/1239
##
## - Excel gives the wrong number of rows, but itself has an import
## problem, with false line breaks: compare Excel read on row 60
## around "glass of juice" (likely because of character limitations in
## cells) with text editor (on VIM, there's no character encoding
## causing the line break)
##
## - LibreOffice throws a similar error ("The data could not be loaded
## completely because the maximum number of characters per cell was
## exceeded"), but yields 13,299 rows
##
dta <- fread("YelpBias/KangData/instances_mergerd_seattle.csv", verbose=F)
length(dta)
nrow(dta)
year <- as.numeric(substr(dta$inspection_period_end_date,start=1,stop=4))
dta <- data.frame(dta, year=year)
## One coding error
dta$inspection_prev_penalty_score[which(dta$inspection_prev_penalty_score==-1)] <- 0
dta$inspection_average_prev_penalty_scores[which(dta$inspection_average_prev_penalty_scores==-1)] <- 0
## Variable names
names(dta)
# [1] "inspection_id"
# [2] "restaurant_id"
# [3] "inspection_period_start_date"
# [4] "inspection_period_end_date"
# [5] "inspection_penalty_score"
# [6] "inspection_average_prev_penalty_scores"
# [7] "inspection_prev_penalty_score"
# [8] "cuisines"
# [9] "zip_code"
#[10] "review_count"
#[11] "non_positive_review_count"
#[12] "average_review_rating"
#[13] "review_contents"
## Some notes about data:
##
## - Reviews (review_contents) are aggregated by period, so it's
## important to check for robustness to review_count
##
## - As best as we can tell from comparison to the King County data,
## inspection_average_prev_penalty_scores scopes back 1 round and
## averages across ALL prior inspection cycles
##
##
## What exactly does inspection_average_prev_penalty_scores
## represent?  My sense is that at least one cycle is omitted to
## calculate the score in period 1
##
## Let's check against King County data
##
#insp <- fread("King_DB/3Master Datasets/master_inspections.csv")
#bus <- fread("King_DB/3Master Datasets/master_businesses.csv")
insp <- fread("~/Dropbox (Stanford Law School)/King_DB/3Master Datasets/master_inspections.csv")
bus <- fread("~/Dropbox (Stanford Law School)/King_DB/3Master Datasets/master_businesses.csv")
## Sort inspection reverse chronologically
insp <- insp[order(business_record_id, insp$inspection_date,decreasing=T),]
## Randomly examining some inspection histories of establishments to understand how average is calculated
which.id <- "DA2383404"
kang.sub <- dta[dta$restaurant_id==dta$restaurant_id[which(dta$inspection_id==which.id)],
c("inspection_period_end_date","inspection_penalty_score",
"inspection_average_prev_penalty_scores","inspection_prev_penalty_score")]
king.sub <- insp[insp$business_record_id==insp$business_record_id[insp$inspection_serial_num==which.id] &
insp$inspection_type==128 &
insp$inspection_date <= kang.sub$inspection_period_end_date[1],
c("inspection_date","inspection_score")]
#2012-10-02 -- actual inspection_penalty_score seems to be wrong (35 in King County Data, 0 in Kang)
#Unclear how average of 22 was calculated in first observation of that establishment
## This one seems to scope back only one routine
which.id <- "DAWE0JV4G"
kang.sub <- dta[dta$restaurant_id==dta$restaurant_id[which(dta$inspection_id==which.id)],
c("inspection_period_end_date","inspection_penalty_score",
"inspection_average_prev_penalty_scores","inspection_prev_penalty_score")]
king.sub <- insp[insp$business_record_id==insp$business_record_id[insp$inspection_serial_num==which.id] &
insp$inspection_type==128 &
insp$inspection_date <= kang.sub$inspection_period_end_date[1],
c("inspection_date","inspection_score"), with=F]
## This one also scopes back only one routine
which.id <- "DA2143321"
kang.sub <- dta[dta$restaurant_id==dta$restaurant_id[which(dta$inspection_id==which.id)],
c("inspection_period_end_date","inspection_penalty_score",
"inspection_average_prev_penalty_scores","inspection_prev_penalty_score")]
king.sub <- insp[insp$business_record_id==insp$business_record_id[insp$inspection_serial_num==which.id] &
insp$inspection_type==128 &
insp$inspection_date <= kang.sub$inspection_period_end_date[1],
c("inspection_date","inspection_score"), with=F]
##
## Identify terms that indicate food poisoning (see NYC Yelp piece)
##
poison <- as.numeric(grepl("food poisoning",dta$review_contents))
vomit <- as.numeric(grepl("vomit",dta$review_contents))
diarrhea <- as.numeric(grepl("diarrhea", dta$review_contents))
sick <- as.numeric(grepl("sick", dta$review_contents))
homesick <- as.numeric(grepl("homesick", dta$review_contents))
## Robustness to mentions of homesickness
sick2 <- as.numeric(sick==1 & homesick==0)
fbi <- as.numeric((poison+vomit+diarrhea+sick)>0)
fbi2 <- as.numeric((poison+vomit+diarrhea+sick2)>0)
## Other terms
gross <- as.numeric(grepl("gross",dta$review_contents))
sketch <- as.numeric(grepl("sketch",dta$review_contents))##***
rude <- as.numeric(grepl("rude",dta$review_contents))
disgust <- as.numeric(grepl("disgust",dta$review_contents))
sanit <- as.numeric(grepl("sanitation",dta$review_contents))
dirty <- as.numeric(grepl("dirty",dta$review_contents))
clean <- as.numeric(grepl("cleanliness",dta$review_contents)) #*** Floors, Walls, Ceiling
fwc <- as.numeric((sketch+gross+rude+disgust+sanit+clean+dirty)>0)
## Augmenting data frame
dta <- data.frame(dta, poison = poison, vomit = vomit, diarrhea = diarrhea, sick = sick,
sick2=sick2, fbi = fbi, fbi2 = fbi2, gross=gross, sketch=sketch, rude=rude,
disgust=disgust, sanit=sanit, dirty=dirty, clean = clean, fwc=fwc)
##
## Cuisine classification
##
cuisines <- unique(dta$cuisines)
cuisines <- gsub("\\[|\\]|'","", cuisines)
cuisines <- strsplit(cuisines,", ")
cuisines <- unique(unlist(cuisines))
## Formatting as a matrix
cuisine.mat <- data.frame(matrix(NA, nrow(dta), length(cuisines)))
names(cuisine.mat) <- cuisines
for(i in 1:length(cuisines)){
cuisine.mat[,i] <- as.numeric(grepl(cuisines[i], dta$cuisines))
}
## Distribution
round(apply(cuisine.mat,2,mean),2)
##labels <- read.csv("YelpBias/KangData/cuisines.csv", header=T, as.is=T)
labels <- read.csv("~/Dropbox (Stanford Law School)/YelpBias/KangData/cuisines.csv", header=T, as.is=T)
## Checking column alignment
cbind(labels[,1], names(cuisine.mat))
## Simple indicator of Asian / Ethnic restaurant
##
asian <- as.numeric(apply(cuisine.mat[,labels$asian==1],1,sum)>0)
ethnic <- as.numeric(apply(cuisine.mat[,labels$ethnic==1],1,sum)>0)
## Augmenting data frame
dta <- data.frame(dta, asian = asian, ethnic = ethnic)
n1 <- sum(dta$asian==1)
n0 <- sum(dta$asian==0)
mean(dta$inspection_prev_penalty_score[dta$asian==1])
sd(dta$inspection_prev_penalty_score[dta$asian==1])/sqrt(n1)
mean(dta$inspection_prev_penalty_score[dta$asian==0])
sd(dta$inspection_prev_penalty_score[dta$asian==0])/sqrt(n0)
t.test(dta$inspection_prev_penalty_score ~ I(dta$asian==1))
mean(dta$inspection_average_prev_penalty_scores[dta$asian==1])
sd(dta$inspection_average_prev_penalty_scores[dta$asian==1])/sqrt(n1)
mean(dta$inspection_average_prev_penalty_scores[dta$asian==0])
sd(dta$inspection_average_prev_penalty_scores[dta$asian==0])/sqrt(n0)
t.test(dta$inspection_average_prev_penalty_scores ~ I(dta$asian==1))
mean(dta$review_count[dta$asian==1])
sd(dta$review_count[dta$asian==1])/sqrt(n1)
mean(dta$review_count[dta$asian==0])
sd(dta$review_count[dta$asian==0])/sqrt(n0)
t.test(review_count ~ I(asian==1), data=dta)
mean(dta$average_review_rating[dta$asian==1])
sd(dta$average_review_rating[dta$asian==1])/sqrt(n1)
mean(dta$average_review_rating[dta$asian==0])
sd(dta$average_review_rating[dta$asian==0])/sqrt(n0)
t.test(average_review_rating ~ I(asian==1), data=dta)
100*mean(dta$fbi2[dta$asian==1])
100*sd(dta$fbi2[dta$asian==1])/sqrt(n1)
100*mean(dta$fbi2[dta$asian==0])
100*sd(dta$fbi2[dta$asian==0])/sqrt(n0)
t.test(fbi2 ~ I(asian==1), data=dta)
labels <- read.csv("YelpBias/KangData/cuisines.csv", header=T, as.is=T)
#labels <- read.csv("~/Dropbox (Stanford Law School)/YelpBias/KangData/cuisines.csv", header=T, as.is=T)
## Checking column alignment
cbind(labels[,1], names(cuisine.mat))
## Simple indicator of Asian / Ethnic restaurant
##
asian <- as.numeric(apply(cuisine.mat[,labels$asian==1],1,sum)>0)
ethnic <- as.numeric(apply(cuisine.mat[,labels$ethnic==1],1,sum)>0)
## Augmenting data frame
dta <- data.frame(dta, asian = asian, ethnic = ethnic)
##=========================================
##  DESCRIPTIVE STATISTICS
##=========================================
## Covariates
n1 <- sum(dta$asian==1)
n0 <- sum(dta$asian==0)
mean(dta$inspection_prev_penalty_score[dta$asian==1])
sd(dta$inspection_prev_penalty_score[dta$asian==1])/sqrt(n1)
mean(dta$inspection_prev_penalty_score[dta$asian==0])
sd(dta$inspection_prev_penalty_score[dta$asian==0])/sqrt(n0)
t.test(dta$inspection_prev_penalty_score ~ I(dta$asian==1))
mean(dta$inspection_average_prev_penalty_scores[dta$asian==1])
sd(dta$inspection_average_prev_penalty_scores[dta$asian==1])/sqrt(n1)
mean(dta$inspection_average_prev_penalty_scores[dta$asian==0])
sd(dta$inspection_average_prev_penalty_scores[dta$asian==0])/sqrt(n0)
t.test(dta$inspection_average_prev_penalty_scores ~ I(dta$asian==1))
mean(dta$review_count[dta$asian==1])
sd(dta$review_count[dta$asian==1])/sqrt(n1)
mean(dta$review_count[dta$asian==0])
sd(dta$review_count[dta$asian==0])/sqrt(n0)
t.test(review_count ~ I(asian==1), data=dta)
mean(dta$average_review_rating[dta$asian==1])
sd(dta$average_review_rating[dta$asian==1])/sqrt(n1)
mean(dta$average_review_rating[dta$asian==0])
sd(dta$average_review_rating[dta$asian==0])/sqrt(n0)
t.test(average_review_rating ~ I(asian==1), data=dta)
100*mean(dta$fbi2[dta$asian==1])
100*sd(dta$fbi2[dta$asian==1])/sqrt(n1)
100*mean(dta$fbi2[dta$asian==0])
100*sd(dta$fbi2[dta$asian==0])/sqrt(n0)
t.test(fbi2 ~ I(asian==1), data=dta)
## Zip code imbalance
tab.out <- matrix(NA, n.zip, 7)
for(i in 1:n.zip){
my.t <- t.test(zip.mat[,i] ~ dta$asian)
tab.out[i,1] <- all.zip[i]
tab.out[i,2] <- my.t$estimate[2]*100
p1 <- mean(zip.mat[,i]==1 & asian==1)
tab.out[i,3] <- sqrt(p1 * (1-p1)/nrow(dta))*100
tab.out[i,4] <- my.t$estimate[1]*100
p0 <- mean(zip.mat[,i]==1 & asian==0)
tab.out[i,5] <- sqrt(p0 * (1-p0)/nrow(dta))*100
tab.out[i,6] <- my.t$p.value
tab.out[i,7] <- sum(zip.mat[,i])
}
tab.out <- tab.out[order(tab.out[,7], decreasing=T),]
tab.out <- data.frame(tab.out)
## To add to balance table
print(xtable(tab.out[tab.out[,7]>1000,1:6], digits = c(0,0,2,2,2,2,2)), include.rownames=FALSE)
## Which ZIP code has sharpest difference between Asian and non-Asian?
## Zip code imbalance
dif.out <- matrix(NA, n.zip, 2)
for(i in 1:n.zip){
my.t <- t.test(dta$inspection_average_prev_penalty_scores ~ zip.mat[,i])
dif.out[i,1] <- all.zip[i]
dif.out[i,2] <- my.t$statistic
}
## 98105
plot(density(dta$inspection_average_prev_penalty_scores[dta$zip98105==1 & dta$asian==0]))
lines(density(dta$inspection_average_prev_penalty_scores[dta$zip98105==1 & dta$asian==1]))
## 98115
plot(density(dta$inspection_average_prev_penalty_scores[dta$zip98115==1 & dta$asian==0]))
lines(density(dta$inspection_average_prev_penalty_scores[dta$zip98115==1 & dta$asian==1]))
## 98144
par(mar=c(3,3,2,1), mgp=c(1.5,0.5,0), tcl-=0.3)
hist(dta$inspection_average_prev_penalty_scores[dta$zip98144==1 & dta$asian==0],
col=rgb(0,0,0,0.4), border="white",xlim=c(0,40),freq=F, xlab="Average prior score", main="Separating Scores")
hist(dta$inspection_average_prev_penalty_scores[dta$zip98144==1 & dta$asian==1],add=T, col=rgb(1,0,0,0.4), border="white", freq=F)
## Prop establishments Asian
mean(dta$asian)
## Cuisine table
food <- data.frame(matrix(NA, length(cuisines), 3))
food[,1] <- cuisines
food[,2] <- labels$asian
food[,3] <- apply(cuisine.mat,2,sum)
names(food) <- c("Cuisine", "Asian", "Count")
food <- food[order(food$Asian, food$Count, decreasing=T),]
rownames(food) <- NULL
print(xtable(food[,c(1,3)],display=c("s","s","d")),include.rownames=F)
## Counts by Asian / non-Asian
## Most of the outcome variability is driven by "sick"
row1 <- apply(dta[dta$asian==1,c("poison", "vomit", "diarrhea", "sick2")],2,sum)
row2 <- apply(dta[dta$asian==0,c("poison", "vomit", "diarrhea", "sick2")],2,sum)
row3 <- apply(dta[dta$asian==1,c("poison", "vomit", "diarrhea", "sick2")],2,sum)/apply(dta[,c("poison", "vomit", "diarrhea", "sick2")],2,sum)
xtable(rbind(row1,row2,row3))
threshold <- 100
table.zip <- table(dta$zip_code, dta$asian)
unique.zip <- row.names(table.zip)[table.zip[,1]>threshold & table.zip[,2]>threshold]
table.zip <- table.zip[table.zip[,1]>threshold & table.zip[,2]>threshold,]
n.zip <- length(unique.zip)
betas <- matrix(NA, n.zip, 2)
for(i in 1:n.zip){
lm.tmp <- glm(fbi ~ asian + inspection_average_prev_penalty_scores + as.factor(year),
family="binomial", data=dta[dta$zip_code==unique.zip[i],])
betas[i, 1:2] <- coef(summary(lm.tmp))[2,1:2]
}
prop.asian <- table.zip[,2]/apply(table.zip,1,sum)
par(mar=c(3,3,2,1),mgp=c(1.5,0.5,0),tcl=-0.3)
plot(prop.asian, betas[,1], pch=16, ylim=c(-3,3),
cex=sqrt(table.zip[,2]/pi)*0.2,
col=rgb(0,0,0,0.4), main="Effect by ZIP Code",
xlab = "Proportion Asian", ylab="Coefficient")
segments(prop.asian, betas[,1]+betas[,2],
prop.asian, betas[,1]-betas[,2])
abline(h=0,col="darkgrey")
cairo_ps("../figures/ByZIP.eps", height=4,width=5,family = 'Times')
par(mar=c(3,3,2,1),mgp=c(1.5,0.5,0),tcl=-0.3)
plot(prop.asian, betas[,1], pch=16, ylim=c(-3,3),
cex=sqrt(table.zip[,2]/pi)*0.2,
col=rgb(0,0,0,0.4), main="Effect by ZIP Code",
xlab = "Proportion Asian", ylab="Coefficient")
segments(prop.asian, betas[,1]+betas[,2],
prop.asian, betas[,1]-betas[,2])
abline(h=0,col="darkgrey")
dev.off()
dev.off()
dev.off()
dev.off()
cairo_ps('~/Dropbox/YelpBias/JITE/Revisions/figs_eps/ByZIP.eps', height=4,width=5,family = 'Times')
par(mar=c(3,3,2,1),mgp=c(1.5,0.5,0),tcl=-0.3)
plot(prop.asian, betas[,1], pch=16, ylim=c(-3,3),
cex=sqrt(table.zip[,2]/pi)*0.2,
col=rgb(0,0,0,0.4), main="Effect by ZIP Code",
xlab = "Proportion Asian", ylab="Coefficient")
segments(prop.asian, betas[,1]+betas[,2],
prop.asian, betas[,1]-betas[,2])
abline(h=0,col="darkgrey")
dev.off()
## kristen added 8/30 for revisions
cairo_ps('~/Dropbox/YelpBias/JITE/Revisions/figs_eps/ByZIP.eps', height=4,width=5,family = 'Times')
par(mar=c(3,3,2,1),mgp=c(1.5,0.5,0),tcl=-0.3)
plot(prop.asian, betas[,1], pch=16, ylim=c(-3,3),
cex=sqrt(table.zip[,2]/pi)*0.2,
col=rgb(0,0,0,0.4),# main="Effect by ZIP Code",
xlab = "Proportion Asian", ylab="Coefficient")
segments(prop.asian, betas[,1]+betas[,2],
prop.asian, betas[,1]-betas[,2])
abline(h=0,col="darkgrey")
dev.off()
rm(list=ls())
#setwd('/Users/kristen/Dropbox/YelpBias/kristen_sandbox/final_paper_code/JITE_git_upload/Bias/code/a_EthnicBias/')
## Packages
library(data.table) #to read in larger datasets more effectively
library(lfe) # clustered SEs in OLS
library(clusterSEs) # clustered SEs for GLM
library(xtable)
## Read in data
#dta <- fread("../../../../../../KangData/instances_mergerd_seattle.csv", verbose=F)
dta <- fread("../../data/instances_mergerd_seattle.csv", verbose=F)
## Clearing memory
rm(list=ls())
setwd('/Users/kristen/Dropbox/YelpBias/kristen_sandbox/final_paper_code/JITE_git_upload/Bias/code/a_EthnicBias/')
## Packages
library(data.table) #to read in larger datasets more effectively
library(lfe) # clustered SEs in OLS
library(clusterSEs) # clustered SEs for GLM
library(xtable)
## Read in data
#dta <- fread("../../../../../../KangData/instances_mergerd_seattle.csv", verbose=F)
dta <- fread("../../data/instances_mergerd_seattle.csv", verbose=F)
rm(list=ls())
setwd('/Users/kristen/Dropbox/YelpBias/kristen_sandbox/final_paper_code/JITE_git_upload/Bias/code/a_EthnicBias/')
## Packages
library(data.table) #to read in larger datasets more effectively
library(lfe) # clustered SEs in OLS
library(clusterSEs) # clustered SEs for GLM
library(xtable)
## Read in data
dta <- fread("../../../../../../KangData/instances_mergerd_seattle.csv", verbose=F)
#dta <- fread("../../data/instances_mergerd_seattle.csv", verbose=F)
## Augment year
year <- as.numeric(substr(dta$inspection_period_end_date,start=1,stop=4))
dta <- data.frame(dta, year=year)
## One coding error
dta$inspection_prev_penalty_score[which(dta$inspection_prev_penalty_score==-1)] <- 0
dta$inspection_average_prev_penalty_scores[which(dta$inspection_average_prev_penalty_scores==-1)] <- 0
##
## Identify terms that indicate food poisoning (see NYC Yelp piece)
##
poison <- as.numeric(grepl("food poisoning",dta$review_contents))
vomit <- as.numeric(grepl("vomit",dta$review_contents))
diarrhea <- as.numeric(grepl("diarrhea", dta$review_contents))
sick <- as.numeric(grepl("sick", dta$review_contents))
homesick <- as.numeric(grepl("homesick", dta$review_contents))
sick2 <- as.numeric(sick==1 & homesick==0)
dta <- data.frame(dta, poison = poison, vomit = vomit, diarrhea = diarrhea, sick = sick,
sick2=sick2)
## Foodborne illness indicators
fbi <- as.numeric((poison+vomit+diarrhea+sick)>0)
fbi2 <- as.numeric((poison+vomit+diarrhea+sick2)>0)
## Augmenting data frame
dta <- data.frame(dta, poison = poison, vomit = vomit, diarrhea = diarrhea, sick = sick,
sick2=sick2, fbi = fbi, fbi2 = fbi2)
##
## Cuisine classification
##
cuisines <- unique(dta$cuisines)
cuisines <- gsub("\\[|\\]|'","", cuisines)
cuisines <- strsplit(cuisines,", ")
cuisines <- unique(unlist(cuisines))
cuisine.mat <- data.frame(matrix(NA, nrow(dta), length(cuisines)))
names(cuisine.mat) <- cuisines
for(i in 1:length(cuisines)){
cuisine.mat[,i] <- as.numeric(grepl(cuisines[i], dta$cuisines))
}
## Manually augment cuisines with "Asian" and "ethnic"
labels <- read.csv("../../data/cuisines.csv", header=T, as.is=T)
asian <- as.numeric(apply(cuisine.mat[,labels$asian==1],1,sum)>0)
ethnic <- as.numeric(apply(cuisine.mat[,labels$ethnic==1],1,sum)>0)
## Augmenting data frame
dta <- data.frame(dta, asian = asian, ethnic = ethnic)
##
## Analysis of potential ethnic bias in King County data
##
##=========================================
##  SETTING UP DATA
##=========================================
## Clearing memory
rm(list=ls())
setwd('/Users/kristen/Dropbox/YelpBias/kristen_sandbox/final_paper_code/JITE_git_upload/Bias/code/a_EthnicBias/')
## Packages
library(data.table) #to read in larger datasets more effectively
library(lfe) # clustered SEs in OLS
library(clusterSEs) # clustered SEs for GLM
library(xtable)
## Read in data
dta <- fread("../../../../../../KangData/instances_mergerd_seattle.csv", verbose=F)
#dta <- fread("../../data/instances_mergerd_seattle.csv", verbose=F)
## Augment year
year <- as.numeric(substr(dta$inspection_period_end_date,start=1,stop=4))
dta <- data.frame(dta, year=year)
## One coding error
dta$inspection_prev_penalty_score[which(dta$inspection_prev_penalty_score==-1)] <- 0
dta$inspection_average_prev_penalty_scores[which(dta$inspection_average_prev_penalty_scores==-1)] <- 0
##
## Identify terms that indicate food poisoning (see NYC Yelp piece)
##
poison <- as.numeric(grepl("food poisoning",dta$review_contents))
vomit <- as.numeric(grepl("vomit",dta$review_contents))
diarrhea <- as.numeric(grepl("diarrhea", dta$review_contents))
sick <- as.numeric(grepl("sick", dta$review_contents))
homesick <- as.numeric(grepl("homesick", dta$review_contents))
sick2 <- as.numeric(sick==1 & homesick==0)
dta <- data.frame(dta, poison = poison, vomit = vomit, diarrhea = diarrhea, sick = sick,
sick2=sick2)
## Foodborne illness indicators
fbi <- as.numeric((poison+vomit+diarrhea+sick)>0)
fbi2 <- as.numeric((poison+vomit+diarrhea+sick2)>0)
## Augmenting data frame
dta <- data.frame(dta, poison = poison, vomit = vomit, diarrhea = diarrhea, sick = sick,
sick2=sick2, fbi = fbi, fbi2 = fbi2)
##
## Cuisine classification
##
cuisines <- unique(dta$cuisines)
cuisines <- gsub("\\[|\\]|'","", cuisines)
cuisines <- strsplit(cuisines,", ")
cuisines <- unique(unlist(cuisines))
## Formatting as a matrix
cuisine.mat <- data.frame(matrix(NA, nrow(dta), length(cuisines)))
names(cuisine.mat) <- cuisines
for(i in 1:length(cuisines)){
cuisine.mat[,i] <- as.numeric(grepl(cuisines[i], dta$cuisines))
}
## Manually augment cuisines with "Asian" and "ethnic"
labels <- read.csv("../../data/cuisines.csv", header=T, as.is=T)
## Simple indicator of Asian / Ethnic restaurant
##
asian <- as.numeric(apply(cuisine.mat[,labels$asian==1],1,sum)>0)
ethnic <- as.numeric(apply(cuisine.mat[,labels$ethnic==1],1,sum)>0)
## Augmenting data frame
dta <- data.frame(dta, asian = asian, ethnic = ethnic)
n1 <- sum(dta$asian==1)
n0 <- sum(dta$asian==0)
mean(dta$inspection_prev_penalty_score[dta$asian==1])
sd(dta$inspection_prev_penalty_score[dta$asian==1])/sqrt(n1)
mean(dta$inspection_prev_penalty_score[dta$asian==0])
sd(dta$inspection_prev_penalty_score[dta$asian==0])/sqrt(n0)
t.test(dta$inspection_prev_penalty_score ~ I(dta$asian==1))
mean(dta$inspection_average_prev_penalty_scores[dta$asian==1])
sd(dta$inspection_average_prev_penalty_scores[dta$asian==1])/sqrt(n1)
mean(dta$inspection_average_prev_penalty_scores[dta$asian==0])
sd(dta$inspection_average_prev_penalty_scores[dta$asian==0])/sqrt(n0)
t.test(dta$inspection_average_prev_penalty_scores ~ I(dta$asian==1))
mean(dta$review_count[dta$asian==1])
sd(dta$review_count[dta$asian==1])/sqrt(n1)
mean(dta$review_count[dta$asian==0])
sd(dta$review_count[dta$asian==0])/sqrt(n0)
t.test(review_count ~ I(asian==1), data=dta)
mean(dta$average_review_rating[dta$asian==1])
sd(dta$average_review_rating[dta$asian==1])/sqrt(n1)
mean(dta$average_review_rating[dta$asian==0])
sd(dta$average_review_rating[dta$asian==0])/sqrt(n0)
t.test(average_review_rating ~ I(asian==1), data=dta)
## Zip code imbalance
all.zip <- unique(dta$zip_code)
n.zip <- length(all.zip)
zip.mat <- matrix(NA, nrow(dta), n.zip)
for(i in 1:n.zip){
zip.mat[,i] <- as.numeric(dta$zip_code==all.zip[i])
}
zip.mat <- data.frame(zip.mat)
names(zip.mat) <- paste("zip",all.zip, sep="")
dta <- data.frame(dta, zip.mat)
threshold <- 100
table.zip <- table(dta$zip_code, dta$asian)
unique.zip <- row.names(table.zip)[table.zip[,1]>threshold & table.zip[,2]>threshold]
table.zip <- table.zip[table.zip[,1]>threshold & table.zip[,2]>threshold,]
n.zip <- length(unique.zip)
betas <- matrix(NA, n.zip, 2)
for(i in 1:n.zip){
lm.tmp <- glm(fbi ~ asian + inspection_average_prev_penalty_scores + as.factor(year),
family="binomial", data=dta[dta$zip_code==unique.zip[i],])
betas[i, 1:2] <- coef(summary(lm.tmp))[2,1:2]
}
prop.asian <- table.zip[,2]/apply(table.zip,1,sum)
cairo_ps('~/Dropbox/YelpBias/JITE/Revisions/figs_eps/ByZIP.eps', height=4,width=5,family = 'Times')
par(mar=c(3,3,2,1),mgp=c(1.5,0.5,0),tcl=-0.3)
plot(prop.asian, betas[,1], pch=16, ylim=c(-3,3),
cex=sqrt(table.zip[,2]/pi)*0.2,
col=rgb(0,0,0,0.4),# main="Effect by ZIP Code",
xlab = "Proportion Asian", ylab="Coefficient")
segments(prop.asian, betas[,1]+betas[,2],
prop.asian, betas[,1]-betas[,2])
abline(h=0,col="darkgrey")
dev.off()
