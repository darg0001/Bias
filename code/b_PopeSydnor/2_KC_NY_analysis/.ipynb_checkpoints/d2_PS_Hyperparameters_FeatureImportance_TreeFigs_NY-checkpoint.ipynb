{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6/23/2018\n",
    "\n",
    "# P&S applied to New York 311 Call Complaint Data.\n",
    "This notebook plotss feature importaance from fitted RF model on random 80-20 train/test split and also plots one tree from fitted model. These figures correspond with the appendix figures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(51470, 5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kristen/anaconda/lib/python2.7/site-packages/pandas/core/generic.py:3110: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self[name] = value\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "from IPython.display import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "\n",
    "\n",
    "execfile('../../functions/python_libraries.py')\n",
    "execfile('../../functions/simulation_functions.py')\n",
    "\n",
    "ny_df = pd.read_csv('../../../data/NYData.csv')\n",
    "\n",
    "ny_df_subset = ny_df[['BORO', 'SCORE', 'complaint','Asian','subsequentScore', 'avg.score']]\n",
    "\n",
    "\n",
    "ny_df_subset.reset_index(inplace=True, drop=True)\n",
    "\n",
    "ny_df_subset.columns = ['BORO_SAP', 'score_SAP', 'complaint_CP',\n",
    "                        'asian_SUP','y','avg_score_SAP']#, 'non_chinese_SUP']\n",
    "\n",
    "\n",
    "\n",
    "ny_df_subset.BORO_SAP = map(str,ny_df_subset.BORO_SAP)\n",
    "ny_df_subset = ny_df_subset.loc[~ny_df_subset.y.isnull()]\n",
    "ny_df_subset = ny_df_subset.loc[ny_df_subset.y != -1]\n",
    "\n",
    "## try one-hop encoding CAMIS and year_SAP; unclear how RF is handling these categorical features\n",
    "zip_SAP = pd.get_dummies(ny_df_subset.BORO_SAP)\n",
    "\n",
    "## subset to only 4-columns -- (i.e. K-1 for K categories)\n",
    "#years_SAP = years_SAP[[u'BROOKLYN', u'MANHATTAN', u'QUEENS', u'STATEN ISLAND']]\n",
    "\n",
    "zip_SAP.columns = np.array(zip_SAP.columns) + '_SAP'\n",
    "print np.shape(zip_SAP)\n",
    "ny_df_subset_RF = pd.concat((ny_df_subset,zip_SAP), axis = 1)\n",
    "SAP_cols_RF = [s for s in np.array(ny_df_subset_RF.columns) if \"SAP\" in s]\n",
    "CP_cols_RF = ['complaint_CP']\n",
    "\n",
    "zip_SAP = pd.get_dummies(ny_df_subset.BORO_SAP)\n",
    "zip_SAP = zip_SAP[np.array(zip_SAP.columns)[0:len(np.array(zip_SAP.columns))-1]]\n",
    "zip_SAP.columns = np.array(zip_SAP.columns) + '_SAP'\n",
    "ny_df_subset_OLS = pd.concat((ny_df_subset,zip_SAP), axis = 1)\n",
    "SAP_cols_OLS = [s for s in np.array(ny_df_subset_OLS.columns) if \"SAP\" in s]\n",
    "CP_cols_OLS= ['complaint_CP']\n",
    "\n",
    "\n",
    "ny_df_subset_RF.drop(['BORO_SAP'], inplace = True, axis = 1)\n",
    "ny_df_subset_OLS.drop(['BORO_SAP'], inplace = True, axis = 1)\n",
    "\n",
    "\n",
    "\n",
    "SAP_cols_OLS = [s for s in np.array(ny_df_subset_OLS.columns) if \"SAP\" in s]\n",
    "SAP_cols_RF = [s for s in np.array(ny_df_subset_RF.columns) if \"SAP\" in s]\n",
    "\n",
    "\n",
    "ny_df_subset_RF.drop_duplicates(inplace = True)\n",
    "ny_df_subset_OLS.drop_duplicates(inplace = True)\n",
    "ny_df_subset_RF.y = map(np.int,ny_df_subset_RF.y)\n",
    "ny_df_subset_OLS.y = map(np.int,ny_df_subset_OLS.y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One Train/Test Split for RF Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf = sklearn.ensemble.RandomForestRegressor()\n",
    "max_depth = [3, 5, 10]\n",
    "max_depth.append(None) \n",
    "min_samples_leaf = [5, 10, 20, 50, 100]\n",
    "min_samples_split = [2, 3, 4, 5, 10] \n",
    "n_estimators = [50, 100, 150] #[10, 50, 100, 150, 200]\n",
    "max_features = ['auto', 0.25, 0.5, 0.75] \n",
    "random_grid = {'max_depth': max_depth,\n",
    "            'min_samples_leaf': min_samples_leaf,\n",
    "              'max_features': max_features,\n",
    "              'n_estimators': n_estimators,\n",
    "              'min_samples_split': min_samples_split}\n",
    "\n",
    "model = GridSearchCV(estimator = clf,\n",
    "                param_grid = random_grid,\n",
    "                cv = 3, verbose=0,\n",
    "                n_jobs = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kristen/anaconda/lib/python2.7/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import sklearn.cross_validation\n",
    "k_fold = sklearn.cross_validation.ShuffleSplit(len(ny_df_subset_RF.y), n_iter=1,\n",
    "                                                         test_size=0.2,\n",
    "                                                         random_state=0)\n",
    "\n",
    "for k, (train, test) in enumerate(k_fold):\n",
    "    print k\n",
    "    df_subset_train = ny_df_subset_RF.iloc[train,:]\n",
    "    df_subset_test = ny_df_subset_RF.iloc[test,:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Justify Hyperparameter Grid Range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Hyperparamter Search set-up\n",
    "clf = sklearn.ensemble.RandomForestRegressor()\n",
    "max_depth = [3, 5, 10]\n",
    "max_depth.append(None) \n",
    "min_samples_leaf = [5, 10, 20, 50, 100]\n",
    "min_samples_split = [2, 3, 4, 5, 10] \n",
    "n_estimators = [50, 100, 150] #[10, 50, 100, 150, 200]\n",
    "max_features = ['auto', 0.25, 0.5, 0.75] \n",
    "random_grid = {'max_depth': max_depth,\n",
    "            'min_samples_leaf': min_samples_leaf,\n",
    "              'max_features': max_features,\n",
    "              'n_estimators': n_estimators,\n",
    "              'min_samples_split': min_samples_split}\n",
    "\n",
    "model = GridSearchCV(estimator = clf,\n",
    "                param_grid = random_grid,\n",
    "                cv = 3, verbose=0,\n",
    "                n_jobs = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kristen/anaconda/lib/python2.7/site-packages/sklearn/ensemble/forest.py:724: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n"
     ]
    }
   ],
   "source": [
    "#http://scikit-learn.org/stable/auto_examples/ensemble/plot_ensemble_oob.html\n",
    "from collections import OrderedDict\n",
    "RANDOM_STATE = 123\n",
    "\n",
    "ensemble_clfs = [\n",
    "    (\"RF, max_depth=3\",\n",
    "        sklearn.ensemble.RandomForestRegressor(warm_start=True, oob_score=True,max_depth = 3,\n",
    "                               max_features='auto',\n",
    "                               random_state=RANDOM_STATE)),\n",
    "    (\"RF, max_depth=5\",\n",
    "        sklearn.ensemble.RandomForestRegressor(warm_start=True, max_features='auto',max_depth = 5,\n",
    "                               oob_score=True,\n",
    "                               random_state=RANDOM_STATE)),\n",
    "    (\"RF, max_depth=10\",\n",
    "        sklearn.ensemble.RandomForestRegressor(warm_start=True, max_features='auto',max_depth = 10,\n",
    "                               oob_score=True,\n",
    "                               random_state=RANDOM_STATE))\n",
    "]\n",
    "\n",
    "error_rate = OrderedDict((label, []) for label, _ in ensemble_clfs)\n",
    "\n",
    "\n",
    "\n",
    "# Range of `n_estimators` values to explore.\n",
    "min_estimators = 5\n",
    "max_estimators = 200\n",
    "\n",
    "for label, clf in ensemble_clfs:\n",
    "    for i in range(min_estimators, max_estimators + 1, 5):\n",
    "        #print i\n",
    "        clf.set_params(n_estimators=i)\n",
    "        clf.fit(df_subset_train[sum([SAP_cols_RF, CP_cols_RF, ['asian_SUP']], [])], \n",
    "                    df_subset_train.y)\n",
    "\n",
    "        # Record the OOB error for each `n_estimators=i` setting.\n",
    "        oob_error = 1 - clf.oob_score_\n",
    "        error_rate[label].append((i, oob_error))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Generate the \"OOB error rate\" vs. \"n_estimators\" plot.\n",
    "for label, clf_err in error_rate.items():\n",
    "    xs, ys = zip(*clf_err)\n",
    "    plt.plot(xs, ys, label=label)\n",
    "    plt.scatter(xs, ys)#, label=label)\n",
    "\n",
    "plt.axvline(50, color = 'gray', alpha = 0.5, ls = '--')\n",
    "plt.axvline(150, color = 'gray', alpha = 0.5, ls = '--')\n",
    "\n",
    "\n",
    "plt.xlim(min_estimators, max_estimators)\n",
    "#plt.ylim(0.88, 0.96)\n",
    "plt.xlabel(\"n_estimators\")\n",
    "plt.ylabel(\"OOB error rate\")\n",
    "plt.title('New York (n_estimators)')\n",
    "#plt.legend(loc=\"upper right\")\n",
    "\n",
    "plt.tight_layout()\n",
    "pp = PdfPages(\"./figs/NY_n_estimators_all_other_parameters_default.pdf\")\n",
    "pp.savefig()\n",
    "pp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.fit(df_subset_train[sum([SAP_cols_RF, CP_cols_RF, ['asian_SUP']], [])], \n",
    "                    df_subset_train.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print sum([SAP_cols_RF, CP_cols_RF, ['asian_SUP']], [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "imp = model.best_estimator_.feature_importances_\n",
    "\n",
    "## here is to rename for figure formatting -- needs to be in same order as above printed\n",
    "names = ['prior score',\n",
    "            'avg. prior score',                     \n",
    "                        'Bronx',\n",
    "                        'Brooklyn',\n",
    "                        'Manhattan',\n",
    "                        'Queens',\n",
    "                        'Staten Island',\n",
    "                        'complaints',\n",
    "                        'is Asian']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "imp, names = zip(*sorted(zip(imp,names)))\n",
    "\n",
    "%matplotlib inline\n",
    "plt.barh(range(len(names)), imp, align = 'center', alpha = 0.5, edgecolor='white', color = 'gray')\n",
    "plt.yticks(range(len(names)), names)\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.title('New York City')\n",
    "plt.ylabel('Features')\n",
    "plt.xlim(0,0.9)\n",
    "plt.tight_layout()\n",
    "pp = PdfPages('./figs/NY_feature_importance.pdf')\n",
    "pp.savefig()\n",
    "pp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize Decision Tree from RF Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tree_id = 89"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from sklearn.externals.six import StringIO  \n",
    "from IPython.display import Image  \n",
    "from sklearn.tree import export_graphviz\n",
    "import pydotplus\n",
    "dot_data = StringIO()\n",
    "export_graphviz(model.best_estimator_.estimators_[tree_id], out_file=dot_data,  \n",
    "                feature_names = ['past inspection',\n",
    "                                 'avg. past inspection',\n",
    "                        'in Bronx',\n",
    "                        'in Brooklyn',\n",
    "                        'in Manhattan',\n",
    "                        'in Queens',\n",
    "                        'in Staten_Island',\n",
    "                        'complaint count',\n",
    "                        'is Asian'],\n",
    "                max_depth = 2,\n",
    "                filled=True, rounded=True,\n",
    "                special_characters=True)\n",
    "graph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \n",
    "Image(graph.create_png())\n",
    "graph.write_pdf(\"./figs/NYC_DT_full_depth_2.pdf\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from sklearn.externals.six import StringIO  \n",
    "from IPython.display import Image  \n",
    "from sklearn.tree import export_graphviz\n",
    "import pydotplus\n",
    "dot_data = StringIO()\n",
    "export_graphviz(model.best_estimator_.estimators_[tree_id], out_file=dot_data,  \n",
    "                feature_names = ['past inspection',\n",
    "                                 'avg. past inspection',\n",
    "                        'in Bronx',\n",
    "                        'in Brooklyn',\n",
    "                        'in Manhattan',\n",
    "                        'in Queens',\n",
    "                        'in Staten_Island',\n",
    "                        'complaint count',\n",
    "                        'is Asian'],\n",
    "                max_depth = 3,\n",
    "                filled=True, rounded=True,\n",
    "                special_characters=True)\n",
    "graph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \n",
    "Image(graph.create_png())\n",
    "graph.write_pdf(\"./figs/NYC_DT_full_depth_3.pdf\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from sklearn.externals.six import StringIO  \n",
    "from IPython.display import Image  \n",
    "from sklearn.tree import export_graphviz\n",
    "import pydotplus\n",
    "dot_data = StringIO()\n",
    "export_graphviz(model.best_estimator_.estimators_[tree_id], out_file=dot_data,  \n",
    "                feature_names = ['past inspection',\n",
    "                                 'avg. past inspection',\n",
    "                                 \n",
    "                        'in Bronx',\n",
    "                        'in Brooklyn',\n",
    "                        'in Manhattan',\n",
    "                        'in Queens',\n",
    "                        'in Staten_Island',\n",
    "                        'complaint count',\n",
    "                        'is Asian'],\n",
    "                #max_depth = 4,\n",
    "                filled=True, rounded=True,\n",
    "                special_characters=True)\n",
    "graph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \n",
    "Image(graph.create_png())\n",
    "graph.write_pdf(\"./figs/NYC_DT_full.pdf\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
